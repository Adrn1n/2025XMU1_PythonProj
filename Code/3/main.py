"""
Optimized main application module for Baidu-Ollama integration.
Provides a streamlined command-line interface using the refactored OllamaIntegrate class.
"""

import argparse
import asyncio
import logging
import sys
from pathlib import Path
from typing import Any, Dict

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent))

# Import the optimized integration class
from ollama.ollama_integrate import (
    create_from_args,
    show_usage_examples,
)

# Import configuration and logging utilities
from config import OLLAMA_CONFIG, get_module_logger
from utils.logging_utils import get_log_level_from_string

# Setup module-specific logger
logger = get_module_logger(__name__)


def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Baidu Search + Ollama LLM Integration (Optimized)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s "artificial intelligence trends"
  %(prog)s "machine learning" -m llama3 -p 2 --debug
  %(prog)s -i  # Interactive mode
        """,
    )

    # Basic arguments
    basic_group = parser.add_argument_group("Basic Options")
    basic_group.add_argument(
        "query",
        nargs="?",
        help="Search query or question (if not specified, you'll be prompted)",
    )
    basic_group.add_argument(
        "-i", "--interactive", action="store_true", help="Run in interactive mode"
    )
    basic_group.add_argument(
        "-m",
        "--model",
        help="Ollama model to use (if not specified, you'll be prompted to select)",
    )
    basic_group.add_argument(
        "--question",
        help="Specific question to ask the LLM (if different from search query)",
    )

    # Search configuration
    search_group = parser.add_argument_group("Search Options")
    search_group.add_argument(
        "-p",
        "--pages",
        type=int,
        default=5,
        help="Number of pages to scrape (default: 5)",
    )
    search_group.add_argument(
        "--no-filter-ads",
        action="store_false",
        dest="filter_ads",
        help="Disable advertisement filtering (default: enabled)",
    )
    search_group.add_argument(
        "--no-cache",
        action="store_true",
        help="Disable URL caching",
    )
    search_group.add_argument(
        "--clear-cache",
        action="store_true",
        help="Clear existing cache before search",
    )
    search_group.add_argument(
        "--cache-file",
        help="Custom cache file path",
    )

    # Ollama configuration
    ollama_group = parser.add_argument_group("Ollama Options")
    ollama_group.add_argument(
        "--ollama-url",
        default=OLLAMA_CONFIG.get("base_url", "http://localhost:11434"),
        help="Ollama API base URL",
    )
    ollama_group.add_argument(
        "--temperature",
        type=float,
        help="Ollama temperature (uses model-specific defaults if not specified)",
    )
    ollama_group.add_argument(
        "--top-p",
        type=float,
        help="Ollama top-p parameter (uses model-specific defaults if not specified)",
    )
    ollama_group.add_argument(
        "--top-k",
        type=int,
        help="Ollama top-k parameter (uses model-specific defaults if not specified)",
    )
    ollama_group.add_argument(
        "--context-size", type=int, help="Context size (window) for the model"
    )
    ollama_group.add_argument(
        "--max-tokens", type=int, help="Maximum tokens to generate"
    )
    ollama_group.add_argument(
        "--no-stream",
        dest="stream",
        action="store_false",
        default=OLLAMA_CONFIG.get("stream", True),
        help="Disable streaming response (wait for complete response)",
    )

    # Output/logging configuration
    output_group = parser.add_argument_group("Output Options")
    output_group.add_argument("-o", "--output", help="Output file for search results")
    output_group.add_argument(
        "--save-results",
        action="store_true",
        help="Save search results to file (always enabled in debug mode unless --no-save-results is used)",
    )
    output_group.add_argument(
        "--no-save-results",
        action="store_true",
        help="Explicitly disable saving search results (overrides debug mode default)",
    )
    output_group.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode (shows detailed logging and automatically saves results unless --no-save-results is specified)",
    )
    output_group.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Log level",
    )
    output_group.add_argument("--log-file", help="Log file path")
    output_group.add_argument(
        "--no-log-console",
        action="store_false",
        dest="log_console",
        default=True,
        help="Don't log to console",
    )
    output_group.add_argument(
        "--no-log-file",
        action="store_true",
        help="Don't log to file",
    )

    # Advanced options
    advanced_group = parser.add_argument_group("Advanced Options")
    advanced_group.add_argument(
        "--concurrent-pages",
        type=int,
        default=5,
        help="Concurrent pages scraping limit (default: 5)",
    )
    advanced_group.add_argument(
        "--concurrent",
        type=int,
        default=25,
        help="Concurrent requests limit (default: 25)",
    )
    advanced_group.add_argument(
        "--batch-size",
        type=int,
        default=25,
        help="Batch processing size (default: 25)",
    )
    advanced_group.add_argument(
        "--timeout",
        type=int,
        default=3,
        help="Request timeout in seconds (default: 3)",
    )
    advanced_group.add_argument(
        "--retries",
        type=int,
        default=0,
        help="Number of retry attempts for failed requests (default: 0)",
    )
    advanced_group.add_argument(
        "--proxy", action="store_true", help="Use proxy for all requests"
    )

    return parser.parse_args()


async def interactive_mode():
    """Run the application in interactive mode."""
    print("=== Baidu Search + Ollama LLM Integration ===")
    print("æ¬¢è¿ä½¿ç”¨ç™¾åº¦æœç´¢ + Ollama LLM æ•´åˆå·¥å…·ï¼")
    print("è¯·æŒ‰ç…§æç¤ºè¾“å…¥ç›¸å…³ä¿¡æ¯è¿›è¡Œæœç´¢å’Œé—®ç­”ã€‚")
    print("è¾“å…¥ 'quit', 'exit' æˆ– 'q' é€€å‡ºç¨‹åº\n")

    while True:
        try:
            # 1. è¯¢é—®æœç´¢å†…å®¹
            query = input("è¯·è¾“å…¥æœç´¢å†…å®¹: ").strip()
            if query.lower() in ["quit", "exit", "q"]:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not query:
                print("âŒ æœç´¢å†…å®¹ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                continue

            # 2. è¯¢é—®æœç´¢é¡µæ•°
            pages = 1  # é»˜è®¤å€¼ï¼Œé˜²æ­¢å¼‚å¸¸æ—¶æœªå®šä¹‰
            while True:
                pages_input = input("è¯·è¾“å…¥æœç´¢é¡µæ•° (é»˜è®¤: 1): ").strip()
                if not pages_input:
                    pages = 1
                    break
                try:
                    pages = int(pages_input)
                    if pages <= 0:
                        print("âš ï¸ é¡µæ•°å¿…é¡»å¤§äº0ï¼Œè¯·é‡æ–°è¾“å…¥")
                        continue
                    break
                except ValueError:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                    continue

            # 3. è¯¢é—®é—®é¢˜æ˜¯å¦ä¸æœç´¢ä¸€è‡´
            question_same = (
                input("æé—®çš„é—®é¢˜æ˜¯å¦ä¸æœç´¢å†…å®¹ä¸€è‡´ï¼Ÿ(y/[n]): ").strip().lower()
            )
            question = None
            if question_same != "y":
                # 4. å¦‚æœä¸ä¸€è‡´ï¼Œè¯¢é—®æé—®é—®é¢˜
                question = input("è¯·è¾“å…¥è¦å‘AIæé—®çš„å…·ä½“é—®é¢˜: ").strip()
                if not question:
                    print("âš ï¸ å°†ä½¿ç”¨æœç´¢å†…å®¹ä½œä¸ºæé—®é—®é¢˜")
                    question = None

            # åˆ›å»ºå‚æ•°å¯¹è±¡ï¼Œå¼€å¯debugæ¨¡å¼
            args = argparse.Namespace(
                query=query,
                question=question,
                pages=pages,
                interactive=False,
                model=None,  # è®©æ¨¡å‹é€‰æ‹©é€»è¾‘å¤„ç†
                filter_ads=True,
                no_cache=False,
                clear_cache=False,
                cache_file=None,
                ollama_url=OLLAMA_CONFIG.get("base_url", "http://localhost:11434"),
                temperature=None,
                top_p=None,
                top_k=None,
                context_size=None,
                max_tokens=None,
                stream=OLLAMA_CONFIG.get("stream", True),
                output=None,
                save_results=True,  # é»˜è®¤ä¿å­˜ç»“æœ
                no_save_results=False,
                debug=True,  # å¼€å¯debugæ¨¡å¼
                log_level="DEBUG",
                log_file=None,
                log_console=True,
                no_log_file=False,
                concurrent_pages=5,
                concurrent=25,
                batch_size=25,
                timeout=3,
                retries=0,
                proxy=False,
            )

            print(f"\nğŸ” å¼€å§‹æœç´¢: {query}")
            print(f"ğŸ“„ æœç´¢é¡µæ•°: {pages}")
            if question:
                print(f"â“ æé—®é—®é¢˜: {question}")
            else:
                print(f"â“ æé—®é—®é¢˜: {query} (ä½¿ç”¨æœç´¢å†…å®¹)")
            print("ğŸ› è°ƒè¯•æ¨¡å¼: å·²å¼€å¯")
            print("ğŸ’¾ ä¿å­˜ç»“æœ: å·²å¯ç”¨\n")

            # åˆ›å»ºintegratorå®ä¾‹å¹¶è¿è¡Œ
            integrator = create_from_args(args)

            # è®¾ç½®æµå¼è¾“å‡ºå›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
            async def stream_callback(chunk: Dict[str, Any]):
                if "response" in chunk and chunk["response"]:
                    print(chunk["response"], end="", flush=True)

            # è¿è¡Œæ•´åˆç¨‹åº
            response = await integrator.run(
                question=question,
                stream_callback=stream_callback if args.stream else None,
            )

            # å¦‚æœä¸æ˜¯æµå¼è¾“å‡ºï¼Œæ‰“å°å®Œæ•´å“åº”
            if not args.stream and response and "response" in response:
                print(f"\nğŸ¤– AI å›ç­”:\n{response['response']}")

            print("\n" + "=" * 60 + "\n")

        except (EOFError, KeyboardInterrupt):
            print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
            print("è¯·é‡æ–°è¾“å…¥æœç´¢å†…å®¹\n")
            continue


async def single_run_mode(args: argparse.Namespace):
    """Run the application for a single query."""
    if not args.query:
        print("âŒ No query provided. Use -i for interactive mode or provide a query.")
        show_usage_examples()
        return 1

    try:
        # Create integrator
        integrator = create_from_args(args)

        print(f"ğŸ” Searching for: {args.query}")
        if args.question:
            print(f"â“ Question: {args.question}")

        # Setup streaming callback if enabled
        async def stream_callback(chunk: Dict[str, Any]):
            if "response" in chunk and chunk["response"]:
                print(chunk["response"], end="", flush=True)

        # Run the integration
        response = await integrator.run(
            question=args.question,
            stream_callback=stream_callback if args.stream else None,
        )

        # Print response if not streaming
        if not args.stream and response and "response" in response:
            print(f"\nğŸ¤– AI Response:\n{response['response']}")

        print("\nâœ… Completed successfully!")
        return 0

    except Exception as e:
        print(f"âŒ Error: {e}")
        return 1


async def main():
    """Main function."""
    # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰æä¾›ä»»ä½•å‚æ•°ï¼ˆç›´æ¥è¿è¡Œï¼‰æˆ–ä½¿ç”¨äº†-iå‚æ•°
    if len(sys.argv) == 1:
        await interactive_mode()
        return 0

    args = parse_args()

    # å¦‚æœä½¿ç”¨äº†-iå‚æ•°ï¼Œä¹Ÿè¿›å…¥äº¤äº’æ¨¡å¼
    if args.interactive:
        await interactive_mode()
        return 0

    # Debug mode adjustments
    if args.debug:
        args.log_level = "DEBUG"
        if not args.no_save_results:
            args.save_results = True

    # Configure root logger to ensure all logs are displayed
    log_level = get_log_level_from_string(args.log_level)
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
        force=True,  # Force reconfiguration
    )

    try:
        return await single_run_mode(args)
    except KeyboardInterrupt:
        print("\n\nOperation cancelled by user.")
        return 1
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        if args.debug:
            import traceback

            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
